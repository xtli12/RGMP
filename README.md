## ğŸŒŸRGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation ğŸ¤–âœ¨
#### An end-to-end framework that unifies geometric-semantic skill reasoning with data-efficient visuomotor control
### ğŸ¤ Human-Robot Interaction Videos ğŸ¥
#### ğŸ‘€ For the full video with sound, please refer to this [link](https://github.com/user-attachments/assets/5c396c4f-d024-41cc-aa6f-935461931ff5). 

|     **Huamn-robot interaction**     | 
| :---------------------------------: | 
| <img src="figs/Human-robot_interaction.gif" width="680" height="510"/> |

|     **Generalization ability**      | 
| :---------------------------------: | 
| <img src="figs/Generalization_grasping.gif" width="680" height="475"/> |

### ğŸ”¥ RGMP Generalization Performance in Maniskill2 Simulator ğŸš€

|               PlugCharger                 |                MoveBucket              |               PushChair             |              OpenCabinetDoor               |               OpenCabinetDrawer              | 
| :---------------------------------: | :------------------------------: | :--------------------------------: | :------------------------------: | :------------------------------: |
| <img src="figs/PlugCharger.gif" width="170" height="170"/> | <img src="figs/MoveBucket.gif" width="170" height="170"/> | <img src="figs/PushChair.gif" width="170" height="170"/> | <img src="figs/OpenCabinetDoor.gif" width="170" height="170"/>| <img src="figs/OpenCabinetDrawer.gif" width="170" height="170"/>| 



### ğŸ› ï¸ Installation Instructions ğŸš€
### ğŸ”§ Step-by-step Setup
```py
Create and activate a Conda environment
conda create -n GSNet python=3.7 -y
conda activate GSNet
```
Install dependencies
#### Install PyTorch
```py
conda install pytorch==1.8.0 torchvision==0.9.0 cudatoolkit=10.2 -c pytorch
```
### Install additional requirements
```py
pip install timm==0.4.12
pip install opencv-python==4.4.0.46 termcolor==1.1.0 yacs==0.1.8 pyyaml scipy
pip install -r requirement.txt
```

### ğŸ§  Skill Library
```py
The framework supports three core manipulation skills with geometric prior integration:
side_grasp(): Optimized for cylindrical objects (cans, bottles) - performs stable lateral grasping
lift_up(): Specialized for crushed/flat objects - executes overhead lifting in cluttered environments with obstacle avoidance
top_pinch(): Designed for small/thin objects (napkins, cables) - enables precise pinch grasping with fine motor control
```
### ğŸ“‚ File Structure
```py
Humanoid/
â”œâ”€â”€ gss_framework.py          # Core RGMP framework implementation
â”œâ”€â”€ yolo_segmentation.py      # YOLOv8-based object segmentation module
â”œâ”€â”€ skill_library.py          # Robot manipulation skill execution logic
â”œâ”€â”€ handler_chat.py           # Natural language interaction handler with RGMP integration
â”œâ”€â”€ handler_api.py            # Qwen-vl visual-language API interface
â”œâ”€â”€ handler_camera.py         # Real-time camera input processing module
â”œâ”€â”€ handler_speech.py         # Speech recognition and synthesis handler
â”œâ”€â”€ prompts.py                # Prompt templates for multimodal policy guidance
â”œâ”€â”€ main.py                   # Main application entry point
â”œâ”€â”€ skill_train.py            # Skill-specific model training script
â””â”€â”€ requirements.txt          # Project dependencies
```

### âš™ï¸ Configuration

Update configs.yaml with your API credentials:
```py
qwen:
  model_name: "qwen-vl-max-latest"
  api_key: "your_qwen_api_key"
```

### ğŸ‹ï¸ Training
To train custom models for specific manipulation skills:
```py
python skill_train.py --train_folder ./dataset/train/ --valid_folder ./dataset/valid/
```

### ğŸ’» Hardware Requirements
GPU: NVIDIA GPU (RTX 4090 recommended for optimal performance)

VRAM: Minimum 8GB (16GB+ preferred for real-time inference)

Sensors: USB camera (1080p+) for visual input; Audio I/O devices for speech interaction

Robot Platform: Compatible with humanoid manipulators supporting ROS control interface

### ğŸ”Œ API Integration
The framework integrates with state-of-the-art AI services:

Qwen-vl API: For multimodal visual-language understanding and decision making

YOLOv8: For real-time object detection and instance segmentation

### ğŸ§ª Maniskill2 Simulator Setup
#### Install base simulator
```py
pip install mani-skill2
cd maniSkill2-Learn
```
#### Install PyTorch compatible with simulator
```py
conda install pytorch==1.11.0 torchvision==0.12.0 cudatoolkit=11.3 -c pytorch
pip install pytorch3d
pip install ninja
pip install -e .
pip install protobuf==3.19.0

# Configure asset directory
ln -s ../ManiSkill2/data data  # Link asset directory
# Alternatively: export MS2_ASSET_DIR={path_to_maniskill2}/data
```

#### ğŸ”§ SparseConvNet Support (for 3D manipulation)
##### Install dependencies
```py
sudo apt-get install libsparsehash-dev  # For Ubuntu; use `brew install google-sparsehash` for macOS
```

#### Install modified torchsparse
```py
pip install torchsparse@git+https://github.com/lz1oceani/torchsparse.git
```

#### ğŸš€ Deployment Workflow
```py
#1. Convert Demonstrations (Controller Setup)
python -m mani_skill2.trajectory.replay_trajectory \
--traj-path demos/rigid_body/PegInsertionSide-v0/trajectory.h5 \
--save-traj --target-control-mode pd_ee_delta_pose \
--obs-mode none --num-procs 32

#2. Configure Observation Mode
# Replace {ENV_NAME}, {PATH}, and {YOUR_DIR} with actual values
python tools/convert_state.py --env-name {ENV_NAME} --num-procs 1 \
--traj-name {PATH}/trajectory.none.pd_joint_delta_pos.h5 \
--json-name {PATH}/trajectory.none.pd_joint_delta_pos.json \
--output-name {PATH}/trajectory.none.pd_joint_delta_pos_pcd.h5 \
--control-mode pd_joint_delta_pos --max-num-traj -1 --obs-mode pointcloud \
--n-points 1200 --obs-frame base --reward-mode dense --render

#3. Run Environment-Specific Evaluation (Example: MoveBucket)
python maniskill2_learn/apis/run_rl.py configs/brl/bc/pointnet_soft_body.py --work-dir {YOUR_DIR} --gpu-ids 0 --cfg-options \
"env_cfg.env_name=Movebucket" "env_cfg.obs_mode=pointcloud" "env_cfg.n_points=1200" "eval_cfg.num=100" "eval_cfg.save_traj=False" \
"eval_cfg.save_video=True" "eval_cfg.num_procs=10" "env_cfg.control_mode=pd_ee_delta_pose" \
"replay_cfg.buffer_filenames={YOUR_PATH}/trajectory.none.pd_ee_delta_pose_pointcloud.h5" "env_cfg.obs_frame=ee" \
"train_cfg.n_checkpoint=10000" "replay_cfg.capacity=10000" "replay_cfg.num_samples=-1" "replay_cfg.cache_size=1000" "train_cfg.n_updates=500"
```

ğŸ“œ License
This project is intended for research purposes only. Please cite our paper if you use this framework in academic work.


# æœºå™¨äººå¤šæ¨¡æ€æ¨ç†å¤§çªç ´ï¼æ­¦å¤§RGMPæ¡†æ¶ï¼Œå°‘é‡æ•°æ®å°±èƒ½"ä¸¾ä¸€åä¸‰"
äººå½¢æœºå™¨äººæ‰§è¡Œå¤šæ ·åŒ–ä»»åŠ¡çš„æ½œåŠ›å·¨å¤§ï¼Œä½†ä¸»æµæ–¹æ³•ä¾èµ–æµ·é‡æ•°æ®è®­ç»ƒï¼Œå¼‚æ„åœºæ™¯ä¸‹å‡ ä½•æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œå¯¼è‡´æ³›åŒ–å¼±ã€è®­ç»ƒæˆæœ¬é«˜ã€‚å¦‚ä½•åœ¨æœ‰é™ç¤ºæ•™ä¸‹å®ç°æ³›åŒ–æ“ä½œï¼Ÿæ­¦æ±‰å¤§å­¦ç ”ç©¶å›¢é˜Ÿåœ¨AAAI 2026ä¸Šç»™å‡ºäº†ç­”æ¡ˆâ€”â€”æå‡ºRGMPç«¯åˆ°ç«¯æ¡†æ¶ï¼Œèåˆå‡ ä½•è¯­ä¹‰æ¨ç†ä¸ç©ºé—´é«˜æ•ˆæ„ŸçŸ¥ï¼Œè®©æœºå™¨äººæ“ä½œèƒ½åŠ›å®ç°è·¨è¶Šå¼æå‡ã€‚

---

## æ ¸å¿ƒæ¶æ„ï¼šä¸¤å¤§ç»„ä»¶ç ´è§£å…³é”®éš¾é¢˜
RGMPæ¡†æ¶é€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ååŒå·¥ä½œï¼Œæ—¢è§£å†³æŠ€èƒ½é€‰æ‹©æ¨¡ç³Šæ€§ï¼Œåˆå®ç°æ•°æ®é«˜æ•ˆæ§åˆ¶ï¼š

### å‡ ä½•å…ˆéªŒæŠ€èƒ½é€‰æ‹©å™¨ï¼ˆGSSï¼‰
- ä»¥è½»é‡çº§æ–¹å¼å°†å‡ ä½•å¸¸è¯†æ³¨å…¥å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œä»…éœ€20æ¡è§„åˆ™çº¦æŸã€‚
- åŠ¨æ€é€‚é…æœªçŸ¥åœºæ™¯ä¸­çš„æŠ€èƒ½é€‰æ‹©ï¼Œé¿å…æ“ä½œå†³ç­–æ¨¡ç³Šã€‚

### è‡ªé€‚åº”é€’å½’é«˜æ–¯ç½‘ç»œï¼ˆARGNï¼‰
- ç»“åˆæ—‹è½¬ä½ç½®ç¼–ç ä¸è‡ªé€‚åº”è¡°å‡æœºåˆ¶ï¼Œæ„å»ºå¸¦ç©ºé—´è®°å¿†çš„æ„ŸçŸ¥äº¤äº’æ¨¡å‹ã€‚
- ç”¨é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰åˆ†å±‚å»ºæ¨¡6è‡ªç”±åº¦æ“ä½œè½¨è¿¹ï¼Œå°‘é‡æ¼”ç¤ºæ•°æ®å³å¯å®ç°çµå·§æ“ä½œã€‚

---

## å®éªŒç»“æœï¼š87%æˆåŠŸç‡+5å€æ•°æ®æ•ˆç‡
ç ”ç©¶å›¢é˜Ÿåœ¨çœŸå®äººå½¢æœºå™¨äººã€æ¡Œé¢åŒè‡‚å¹³å°åŠManiskill2ä»¿çœŸå¹³å°å®Œæˆç³»ç»Ÿæµ‹è¯•ï¼Œæ ¸å¿ƒè¡¨ç°äº®çœ¼ï¼š

### å…³é”®æ€§èƒ½çªç ´
- ä»…ç”¨40æ¡"èŠ¬è¾¾ç½æŠ“å–"æ¼”ç¤ºæ•°æ®è®­ç»ƒï¼Œé¢å¯¹å¯ä¹ç½ã€å–·é›¾ç“¶ã€çº¸å·¾ã€å˜å½¢ç½ä½“ã€äººæ‰‹ç­‰å…¨æ–°å¯¹è±¡ï¼Œå¹³å‡ä»»åŠ¡æˆåŠŸç‡è¾¾**87%** ï¼Œè¾ƒä¸»æµDiffusion Policyæå‡17ä¸ªç™¾åˆ†ç‚¹ã€‚
- æ•°æ®æ•ˆç‡ç›´æ¥æå‡**5å€**ï¼Œä»…éœ€äº”åˆ†ä¹‹ä¸€æ ·æœ¬å°±èƒ½è¾¾åˆ°åŒç­‰ç”šè‡³æ›´ä¼˜æ€§èƒ½ã€‚

### è·¨åœºæ™¯é²æ£’æ€§éªŒè¯
- åœ¨æ¨æ¤…å­ã€æ’å……ç”µå™¨ã€å¼€æŸœé—¨ç­‰å¤æ‚ä»»åŠ¡ä¸­ï¼Œå±•ç°æœ€ä¼˜è·¨ä»»åŠ¡è¿ç§»èƒ½åŠ›ã€‚
- æ— è®ºç»“æ„åŒ–è¿˜æ˜¯éç»“æ„åŒ–ç¯å¢ƒï¼Œå‡èƒ½ç¨³å®šå‘æŒ¥ï¼Œæ‰“ç ´ä¼ ç»Ÿè§†è§‰è¯­è¨€æ¨¡å‹ç¼ºä¹ç©ºé—´æ„ŸçŸ¥çš„å±€é™ã€‚

---

## åº”ç”¨ä»·å€¼ï¼šè¦†ç›–å¤šåœºæ™¯è§„æ¨¡åŒ–è½åœ°
RGMPæ¡†æ¶çš„æ³›åŒ–èƒ½åŠ›çš„çªç ´ï¼Œè®©äººå½¢æœºå™¨äººçš„åº”ç”¨åœºæ™¯å¤§å¹…æ‹“å±•ï¼š
- å·¥ä¸šé¢†åŸŸï¼šå·¥ä¸šè£…é…ã€ç‰©æµåˆ†æ‹£ç­‰é‡å¤æ“ä½œåœºæ™¯ï¼Œé™ä½æ•°æ®æ ‡æ³¨æˆæœ¬ã€‚
- å®¶åº­åœºæ™¯ï¼šæ—¥å¸¸ç‰©å“ä¼ é€’ã€ç®€å•å®¶åŠ¡è¾…åŠ©ï¼Œé€‚é…å¤šæ ·å®¶å±…ç¯å¢ƒã€‚
- åŒ»ç–—é¢†åŸŸï¼šè½»é‡çº§åŒ»ç–—è¾…åŠ©æ“ä½œï¼Œåº”å¯¹ä¸åŒåŒ»ç–—è®¾å¤‡ä¸åœºæ™¯éœ€æ±‚ã€‚

---

## æœªæ¥æ–¹å‘ï¼šè¿ˆå‘"ä¸€æŠ€é€šç™¾æŠ€"
å›¢é˜Ÿå°†è¿›ä¸€æ­¥æ¢ç´¢åŠŸèƒ½æ³›åŒ–èƒ½åŠ›ï¼Œå®ç°å•ä¸€ç‰©ä½“åŠŸèƒ½å­¦ä¹ åˆ°åŒç±»ä»»åŠ¡æ“ä½œè½¨è¿¹çš„è‡ªåŠ¨æ¨ç†ï¼Œå¤§å¹…é™ä½æ•™å­¦æˆæœ¬ã€‚ç›®å‰ï¼Œé¡¹ç›®ç›¸å…³ä»£ç å·²å¼€æºï¼Œæ¬¢è¿é¢†åŸŸå†…ç ”ç©¶è€…äº¤æµæ¢ç´¢ã€‚

é¡¹ç›®ä¸»é¡µï¼šhttps://github.com/xtli12/RGMP/tree/main

è¦ä¸è¦æˆ‘å¸®ä½ æ•´ç†ä¸€ä»½**RGMPæ¡†æ¶æ ¸å¿ƒæŠ€æœ¯ä¸åº”ç”¨åœºæ™¯æ±‡æ€»è¡¨**ï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥é˜…å…³é”®ä¿¡æ¯ï¼Ÿ
